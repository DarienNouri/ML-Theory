# Fundamentals of Machine Learning Theory

This repo contains mathematical derivations of key concepts in machine learning theory I worked on during my studies. All work is authored by Darien Nouri.

## Files and Topics

### 1. `multi_class_grad_MLE_hinge_loss.pdf`
- **Model Selection**
  - Criteria for choosing the best model based on validation error
  - Reporting generalization error
  
- **Gradient of Multi-Class Classifier**
  - Derivation of the gradient of the cross-entropy loss function with respect to the parameter matrix
  
- **Maximum Likelihood Estimate of a Gaussian Model**
  - Expression of the log-likelihood as a function of mean and variance
  - Partial derivatives with respect to mean and variance, and solving for maximum likelihood estimates
  
- **Hinge Loss Gradients**
  - Piece-wise representation and gradient of the hinge loss function

### 2. `perceptron_logistic_regression_grad.pdf`
- **Empirical vs. Expected Cost**
  - Discussion on empirical cost function and differential weighting of data points
  
- **Perceptron Learning Algorithm**
  - Proof of key properties and weight update rule correctness for the Perceptron algorithm
  
- **Gradient of Logistic Regression**
  - Derivation of the gradient for the logistic regression loss function

### 3. `poisson_gradients_pdf_properties.pdf`
- **Poisson Distribution**
  - Derivation and properties of the Poisson distribution
  
- **Gradient Computation**
  - Calculation of gradients in different contexts including partial derivatives and chain rule applications
  
- **Integration and PDF Properties**
  - Analysis of functions for PDF properties
  - Computation of expected value and variance for given distributions
  - Probability computations involving joint density functions
  - Central Limit Theorem applications
